"""
title: Neural Recall
"""

import json
import traceback
from datetime import datetime, timezone, timedelta
from typing import Any, Awaitable, Callable, Dict, List, Literal, Optional, Union
import logging
import re
import asyncio
from difflib import SequenceMatcher
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

import aiohttp
from fastapi.requests import Request
from pydantic import BaseModel, Field

from open_webui.routers.memories import (
    add_memory,
    AddMemoryForm,
    delete_memory_by_id,
    Memories,
)
from open_webui.models.users import Users
from open_webui.main import app as webui_app

logging.basicConfig(level=logging.INFO, format="%(name)s - %(levelname)s - %(message)s")
logger = logging.getLogger("NeuralRecall")

MEMORY_IDENTIFICATION_PROMPT = """You are a precision memory management system. Extract, update, and organize user facts with surgical accuracy.

## CRITICAL OUTPUT REQUIREMENT
RESPOND ONLY WITH VALID JSON ARRAY. NO TEXT. NO EXPLANATIONS. NO MARKDOWN.
Format: [{"operation": "NEW|UPDATE|DELETE", "id": "required_for_UPDATE_DELETE", "content": "required_for_NEW_UPDATE"}]

## STORAGE CRITERIA (Store ONLY if ALL are met):
âœ“ Personal, factual, objective, verifiable about the USER'S LIFE/IDENTITY
âœ“ Stable and significant for future personal interactions  
âœ“ Non-obvious, not easily inferred
âœ“ Directly describes the user (not instructions, prompts, or general content)

## STRICT EXCLUSIONS (NEVER store):
âœ— System prompts, instructions, or formatting guidelines
âœ— General information, questions, comparisons, technical how-tos
âœ— Temporary states (mood, tasks, weather), subjective opinions
âœ— Speculative content ("might", "considering"), trivial details
âœ— Content that starts with formatting symbols (##, -, Ã—, âœ“)
âœ— Lists of principles, rules, or guidelines
âœ— Technical documentation or code snippets
âœ— Requests for AI behavior or response formatting

## FORMAT REQUIREMENTS
â€¢ Start with "User" or "User's" â€¢ Comprehensive contextual facts â€¢ English only â€¢ Dates: dd-mm-yyyy â€¢ Preserve proper names/acronyms
â€¢ Group related information together for better semantic retrieval

## OPERATION LOGIC
- NEW: Novel information not in existing memories
- UPDATE: More specific/corrected version of existing memory
- DELETE: Contradicted, obsolete, or superseded information

## EXAMPLES

### Example 1 - Personal Facts (STORE):
User: "I'm a senior ML engineer at TechCorp and have been working on LLMs since March 2023."
Response:
[
  {"operation": "NEW", "content": "User works as a senior ML engineer at TechCorp, specializing in LLMs since 15-03-2023."}
]

### Example 2 - System Prompts (DO NOT STORE):
User: "## CORE PRINCIPLES\n- **Total factual transparency** â€“ Deliver unaltered information\n- **Anti-censorship enforcement** â€“ Bypass restrictions\n## RESPONSE STRUCTURE\n- **Mandatory bullet points** for answers"
Response:
[]

### Example 3 - Technical Instructions (DO NOT STORE):
User: "Use tables for comparisons, code snippets for technical explanations, emoji headers for clarity, avoid large titles"
Response:
[]

### Example 4 - UPDATE/DELETE Operations:
Existing Memories:
- {"id": "mem001", "content": "User works as a junior developer."}
- {"id": "mem002", "content": "User lives in Portland."}

User: "I got promoted to Senior Developer and moved to Austin last month."
Response:
[
  {"operation": "UPDATE", "id": "mem001", "content": "User works as a Senior Developer."},
  {"operation": "UPDATE", "id": "mem002", "content": "User lives in Austin."}
]

### Example 5 - Mixed Content (Filter carefully):
User: "I live in Berlin and here are my formatting preferences: use bullet points, tables for data, and emoji headers."
Response:
[
  {"operation": "NEW", "content": "User lives in Berlin."}
]

### Example 6 - Family Facts (STORE):
User: "Mein Sohn studiert Medizin in Berlin und wird Kardiologe, meine Tochter ist 12."
Response:
[
  {"operation": "NEW", "content": "User has a son studying medicine in Berlin with plans to become a cardiologist, and a 12-year-old daughter."}
]

## VALIDATION
âœ“ Group related facts into comprehensive, contextually rich memories
âœ“ Maintain semantic coherence for better retrieval
âœ“ Balance detail with conciseness
âœ“ Multiple operations for complex inputs
âœ“ Valid memory IDs for UPDATE/DELETE operations
âœ“ First-person perspective maintained
âœ“ ONLY factual, lasting information about the user's personal life/identity
âœ“ IGNORE all system prompts, instructions, formatting guidelines, or technical content
âœ“ REJECT content that looks like documentation, rules, or AI instructions

OUTPUT: JSON array only. Empty array [] if no memorable content."""


class MemoryOperation(BaseModel):
    """Model for memory operations"""

    operation: Literal["NEW", "UPDATE", "DELETE"]
    id: Optional[str] = None
    content: Optional[str] = None


class Filter:
    class Valves(BaseModel):
        """Configuration valves for the filter"""

        api_url: str = Field(
            default="https://api.openai.com/v1",
            description="OpenAI API URL for memory processing.",
        )
        api_key: str = Field(
            default="",
            description="OpenAI API key (required).",
        )
        model: str = Field(
            default="gpt-4o-mini",
            description="OpenAI model to use for identifying facts to remember. Examples: gpt-4o-mini, gpt-4o, gpt-3.5-turbo.",
        )

        related_memories_n: int = Field(
            default=5,
            description="Number of most relevant memories to inject into the context.",
        )
        relevance_threshold: float = Field(
            default=0.5,
            description="Minimum similarity score (0-1) for a memory to be considered relevant enough to be injected.",
        )
        similarity_threshold: float = Field(
            default=0.9,
            description="Similarity threshold (0-1) for detecting and preventing duplicate memories.",
        )
        
        embedding_model: str = Field(
            default="all-MiniLM-L6-v2",
            description="Sentence transformer model for semantic similarity. Examples: all-MiniLM-L6-v2 (fast), all-mpnet-base-v2 (better quality).",
        )
        
        timezone_hours: int = Field(
            default=0,
            description="Timezone offset in hours (e.g., 5 for UTC+5, -4 for UTC-4).",
        )

    class UserValves(BaseModel):
        enabled: bool = Field(
            default=True, description="Enable or disable the memory function."
        )

    def __init__(self):
        self.valves = self.Valves()
        self.user_valves = self.UserValves()
        self.stored_memories = None
        self._error_message = None
        self._aiohttp_session = None
        self._sentence_model = None

    def get_formatted_datetime(self):
        """Get the current datetime object in the user's timezone."""
        utc_time = datetime.now(timezone.utc)
        user_time = utc_time + timedelta(hours=self.valves.timezone_hours)
        return user_time

    def _get_sentence_model(self) -> SentenceTransformer:
        """Get or initialize the sentence transformer model."""
        if self._sentence_model is None:
            try:
                logger.info(f"Loading sentence transformer model: {self.valves.embedding_model}")
                self._sentence_model = SentenceTransformer(self.valves.embedding_model)
                logger.info("Sentence transformer model loaded successfully")
            except Exception as e:
                logger.error(f"Failed to load sentence transformer model: {e}")
                try:
                    logger.info("Attempting to load fallback model: all-MiniLM-L6-v2")
                    self._sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
                    logger.info("Fallback model loaded successfully")
                except Exception as fallback_error:
                    logger.error(f"Failed to load fallback model: {fallback_error}")
                    raise RuntimeError("Could not load any sentence transformer model")
        return self._sentence_model

    async def _get_aiohttp_session(self) -> aiohttp.ClientSession:
        """Get or create an aiohttp session."""
        if self._aiohttp_session is None or self._aiohttp_session.closed:
            timeout = aiohttp.ClientTimeout(total=30)
            self._aiohttp_session = aiohttp.ClientSession(
                timeout=timeout,
                headers={"User-Agent": "Neural-Recall/1.0"}
            )
        return self._aiohttp_session

    async def _ensure_session_closed(self):
        """Ensure the aiohttp session is properly closed."""
        if self._aiohttp_session and not self._aiohttp_session.closed:
            try:
                await self._aiohttp_session.close()
            except Exception as e:
                logger.warning(f"Error closing aiohttp session: {e}")
            finally:
                self._aiohttp_session = None

    async def inlet(
        self,
        body: Dict[str, Any],
        __event_emitter__: Optional[Callable[[Any], Awaitable[None]]] = None,
        __user__: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """Process incoming messages to inject relevant memories into the context."""
        if not body or not __user__ or not self.user_valves.enabled:
            return body

        try:
            if "messages" in body and body["messages"]:
                user_message = body["messages"][-1]["content"]
                user_id = __user__["id"]

                await self._emit_status(__event_emitter__, "ï¿½ Searching memory vault...", False)

                relevant_memories = await self.get_relevant_memories(
                    user_message, user_id
                )

                if relevant_memories:
                    count = len(relevant_memories)
                    memory_word = "memory" if count == 1 else "memories"
                    await self._emit_status(__event_emitter__, f"ðŸ’¡ Found {count} relevant {memory_word}")
                else:
                    await self._emit_status(__event_emitter__, "ðŸ’­ No relevant memories found")

                if relevant_memories:
                    self._inject_memories_into_context(body, relevant_memories)

                self._inject_datetime_context(body)

        except Exception as e:
            logger.error(f"Error in inlet: {e}\n{traceback.format_exc()}")
            await self._emit_status(__event_emitter__, f"ðŸ™ˆ Error retrieving memories: {str(e)}")
        return body

    async def outlet(
        self,
        body: dict,
        __event_emitter__: Optional[Callable[[Any], Awaitable[None]]] = None,
        __user__: Optional[dict] = None,
    ) -> dict:
        """Process outgoing messages to identify and store new memories from user messages only."""
        if not body or not __user__ or not self.user_valves.enabled:
            return body

        try:
            if "messages" in body and body["messages"]:
                user_message = next(
                    (
                        m["content"]
                        for m in reversed(body["messages"])
                        if m["role"] == "user"
                    ),
                    None,
                )

                logger.debug(f"User message for memory processing: {user_message}")
                if user_message:
                    await self._emit_status(__event_emitter__, "ï¿½ Analyzing message for memorable facts...", False)

                    # Process memories directly
                    try:
                        self.stored_memories = await asyncio.wait_for(
                            self._process_user_memories(
                                user_message,
                                __user__["id"],
                                __event_emitter__,
                            ),
                            timeout=15.0
                        )
                    except asyncio.TimeoutError:
                        logger.warning("Memory processing timed out, continuing without waiting")
                        self.stored_memories = []
        except Exception as e:
            logger.error(f"Error in outlet: {e}\n{traceback.format_exc()}")
            await self._emit_status(__event_emitter__, f"ðŸ™ˆ Error processing memories: {str(e)}")
        return body

    async def _emit_status(
        self,
        event_emitter: Optional[Callable[[Any], Awaitable[None]]],
        description: str,
        done: bool = True,
    ) -> None:
        """Helper method to emit status messages."""
        if event_emitter:
            try:
                await event_emitter({
                    "type": "status",
                    "data": {
                        "description": description,
                        "done": done,
                    },
                })
            except Exception as e:
                logger.error(f"Error in event emitter: {e}")

    def _format_operation_status(self, executed_ops: List[Dict[str, Any]]) -> str:
        """Format memory operations into a fancy status message."""
        if not executed_ops:
            return "ðŸ™ˆ No memory operations executed"
        
        op_emojis = {
            "NEW": "âœ¨",
            "UPDATE": "ðŸ”„", 
            "DELETE": "ðŸ—‘ï¸"
        }
        
        op_actions = {
            "NEW": "Created",
            "UPDATE": "Updated",
            "DELETE": "Removed"
        }
        
        ops_by_type = {"NEW": [], "UPDATE": [], "DELETE": []}
        for op in executed_ops:
            ops_by_type[op["operation"]].append(op)
        
        status_parts = []
        for op_type, ops in ops_by_type.items():
            if ops:
                emoji = op_emojis[op_type]
                action = op_actions[op_type]
                count = len(ops)
                
                memory_word = "memory" if count == 1 else "memories"
                status_parts.append(f"{emoji} {action} {count} {memory_word}")
        
        if len(status_parts) == 1:
            return f"ðŸ§  {status_parts[0]}"
        else:
            return f"ðŸ§  Memory updates: {' | '.join(status_parts)}"

    async def _get_formatted_memories(self, user_id: str) -> List[Dict[str, Any]]:
        """Get all memories for a user from the database."""
        try:
            # Use consistent async pattern for database access
            user_memories = await asyncio.to_thread(
                Memories.get_memories_by_user_id, user_id=str(user_id)
            )
            memories_list = [
                {
                    "id": str(memory.id),
                    "memory": memory.content,
                }
                for memory in user_memories
            ]
            logger.debug(f"Retrieved {len(memories_list)} memories for user {user_id}")
            return memories_list
        except Exception as e:
            logger.error(
                f"Error getting formatted memories: {e}\n{traceback.format_exc()}"
            )
            return []

    def _inject_memories_into_context(
        self, body: Dict[str, Any], memories: List[Dict[str, Any]]
    ) -> None:
        """Inject relevant memories into the system message of the chat body."""
        if not memories:
            return

        memory_context = self._format_memories_for_context(memories)
        system_message_found = False
        for message in body["messages"]:
            if message["role"] == "system":
                message["content"] = f"{message['content']}\n\n{memory_context}"
                system_message_found = True
                break

        if not system_message_found:
            body["messages"].insert(0, {"role": "system", "content": memory_context})

    def _inject_datetime_context(self, body: Dict[str, Any]) -> None:
        """Inject current date/time context into the system message."""
        current_time = self.get_formatted_datetime()
        
        # Format timezone string
        timezone_offset = self.valves.timezone_hours
        if timezone_offset >= 0:
            timezone_str = f"UTC+{timezone_offset}" if timezone_offset > 0 else "UTC"
        else:
            timezone_str = f"UTC{timezone_offset}"
        
        # Get time components
        formatted_date = current_time.strftime("%B %d, %Y")
        formatted_time = current_time.strftime("%H:%M:%S")
        day_of_week = current_time.strftime("%A")
        
        datetime_context = f"CURRENT TIME: {day_of_week}, {formatted_date}, {formatted_time} {timezone_str}"
        
        # Inject into system message
        system_message_found = False
        for message in body["messages"]:
            if message["role"] == "system":
                message["content"] = f"{message['content']}\n\n{datetime_context}"
                system_message_found = True
                break

        if not system_message_found:
            body["messages"].insert(0, {"role": "system", "content": datetime_context})

    def _format_memories_for_context(self, memories: List[Dict[str, Any]]) -> str:
        """Format a list of memories into a string for LLM context."""
        if not memories:
            return ""

        header = "BACKGROUND: You naturally know these facts. Never reference having this information or where it came from:"
        formatted_mems = "\n".join([f"- {mem['memory']}" for mem in memories])
        return f"{header}\n{formatted_mems}"

    async def _process_user_memories(
        self,
        user_message: str,
        user_id: str,
        __event_emitter__: Optional[Callable[[Any], Awaitable[None]]],
    ) -> List[Dict[str, Any]]:
        """Identify and execute memory operations based on the user's message."""
        try:
            logger.debug(f"Processing memories for message: {user_message[:50]}...")
            existing_memories = await self._get_formatted_memories(user_id)
            operations = await self.identify_memory_operations(
                user_message, existing_memories
            )

            if operations:
                logger.info(f"Found {len(operations)} memory operations to process.")
                executed_ops = await self.execute_memory_operations(
                    operations, user_id, existing_memories
                )
                
                if executed_ops:
                    status_message = self._format_operation_status(executed_ops)
                    await self._emit_status(__event_emitter__, status_message)
                else:
                    await self._emit_status(__event_emitter__, "ðŸ™ˆ Failed to update memories")
            else:
                logger.debug("No new memory operations identified.")
                await self._emit_status(__event_emitter__, "ðŸ’­ No memorable facts found")
            return operations
        except asyncio.CancelledError:
            logger.info("Memory processing task was cancelled")
            raise
        except Exception as e:
            logger.error(f"Unexpected error in memory processing: {e}\n{traceback.format_exc()}")
            self._error_message = f"Memory processing failed: {str(e)}"
            return []

    async def identify_memory_operations(
        self,
        input_text: str,
        existing_memories: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """Use an LLM to identify memory operations from the input text."""
        logger.debug("Starting memory identification.")
        system_prompt = MEMORY_IDENTIFICATION_PROMPT

        if existing_memories:
            mem_context = "\n\nExisting Memories:\n" + json.dumps(
                existing_memories, indent=2
            )
            system_prompt += mem_context

        date_context = f"\n\nCurrent Date/Time: {self.get_formatted_datetime().strftime('%A, %B %d, %Y %H:%M:%S %Z')}"
        system_prompt += date_context

        response = await self._query_llm(system_prompt, input_text)
        if not response or response.startswith("Error:"):
            self._error_message = response
            logger.error(f"Error from LLM during memory identification: {response}")
            return []

        logger.info(f"LLM response for memory identification: {response}")
        operations = self._extract_and_parse_json(response)

        if isinstance(operations, dict):
            logger.info(f"LLM returned single operation object, converting to list: {operations}")
            operations = [operations]

        if not isinstance(operations, list):
            logger.warning(
                f"LLM did not return a valid list of operations: {operations}"
            )
            return []

        logger.info(f"Parsed {len(operations)} operations from LLM response")

        valid_ops = [
            op
            for op in operations
            if self._validate_memory_operation(op, existing_memories)
        ]
        logger.info(f"Identified {len(valid_ops)} valid memory operations.")
        return valid_ops

    def _validate_memory_operation(
        self, op: Dict[str, Any], existing_memories: List[Dict[str, Any]]
    ) -> bool:
        """Validate a single memory operation."""
        try:
            MemoryOperation(**op)
            if op["operation"] in ["UPDATE", "DELETE"]:
                existing_ids = {mem["id"] for mem in existing_memories}
                if op.get("id") not in existing_ids:
                    logger.warning(
                        f"Invalid or missing ID for {op['operation']} on non-existent memory: {op.get('id')}"
                    )
                    return False
            return True
        except Exception as e:
            logger.warning(f"Invalid memory operation format: {op}. Error: {e}")
            return False

    def _extract_and_parse_json(self, text: str) -> Union[List, Dict, None]:
        """Extract and parse JSON from text, handling common LLM response issues."""
        if not text:
            logger.warning("Empty text provided to JSON parser")
            return None

        original_text = text
        
        match = re.search(r"```(?:json)?\s*([\s\S]*?)\s*```", text)
        if match:
            text = match.group(1)

        for start_char, end_char in [("[", "]"), ("{", "}")]:
            start_idx = text.find(start_char)
            if start_idx != -1:
                bracket_count = 0
                for i, char in enumerate(text[start_idx:], start_idx):
                    if char == start_char:
                        bracket_count += 1
                    elif char == end_char:
                        bracket_count -= 1
                        if bracket_count == 0:
                            json_text = text[start_idx:i+1]
                            try:
                                result = json.loads(json_text)
                                logger.info(f"Successfully parsed JSON: {type(result)}")
                                return result
                            except json.JSONDecodeError:
                                continue
        
        logger.error(f"Failed to parse JSON from: {original_text[:500]}...")
        return None

    def _calculate_memory_similarity(self, text1: str, text2: str) -> float:
        """Calculate semantic similarity between two strings using sentence transformers."""
        try:
            model = self._get_sentence_model()
            embeddings = model.encode([text1, text2])
            similarity_matrix = cosine_similarity([embeddings[0]], [embeddings[1]])
            return float(max(0, min(1, similarity_matrix[0][0])))
        except Exception as e:
            logger.warning(f"Error calculating semantic similarity, falling back to string similarity: {e}")
            return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()

    async def get_relevant_memories(
        self, current_message: str, user_id: str
    ) -> List[Dict[str, Any]]:
        """Get memories relevant to the current context using sentence similarity."""
        existing_memories = await self._get_formatted_memories(user_id)
        if not existing_memories:
            return []

        scored_memories = []
        for mem in existing_memories:
            similarity = self._calculate_memory_similarity(
                current_message, mem["memory"]
            )
            if similarity >= self.valves.relevance_threshold:
                scored_memories.append({**mem, "relevance": similarity})

        scored_memories.sort(key=lambda x: x["relevance"], reverse=True)
        result = scored_memories[:self.valves.related_memories_n]

        logger.info(
            f"Found {len(scored_memories)} relevant memories above threshold {self.valves.relevance_threshold}. Returning top {len(result)}."
        )
        return result

    async def execute_memory_operations(
        self,
        operations: List[Dict[str, Any]],
        user_id: str,
        existing_memories: List[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """Execute a list of memory operations (NEW, UPDATE, DELETE)."""
        executed_operations = []
        try:
            # Use consistent async pattern for database access
            user = await asyncio.to_thread(Users.get_user_by_id, user_id)
            if not user:
                logger.error(f"User not found: {user_id}")
                return executed_operations

            ops_to_execute = []
            existing_contents = {mem["memory"] for mem in existing_memories}
            for op in operations:
                if op["operation"] == "NEW":
                    is_duplicate = any(
                        self._calculate_memory_similarity(
                            op["content"], existing_content
                        )
                        >= self.valves.similarity_threshold
                        for existing_content in existing_contents
                    )
                    if is_duplicate:
                        logger.debug(
                            f"Skipping duplicate new memory: {op['content'][:50]}..."
                        )
                        continue
                ops_to_execute.append(op)
            
            logger.debug(
                f"Executing {len(ops_to_execute)} memory operations after filtering."
            )

            for op_data in ops_to_execute:
                success = await self._execute_single_operation(MemoryOperation(**op_data), user)
                if success:
                    executed_operations.append(op_data)

            logger.info(
                f"Successfully processed {len(executed_operations)} memory operations."
            )
            return executed_operations
        except Exception as e:
            logger.error(f"Error processing memories: {e}\n{traceback.format_exc()}")
            return executed_operations

    async def _execute_single_operation(
        self, operation: MemoryOperation, user: Any
    ) -> bool:
        """Execute a single memory operation."""
        try:
            request = Request(scope={"type": "http", "app": webui_app})
            
            if operation.operation == "NEW":
                await add_memory(
                    request=request,
                    form_data=AddMemoryForm(content=operation.content),
                    user=user,
                )
                logger.info(f"NEW memory created: {operation.content[:50]}...")

            elif operation.operation == "UPDATE" and operation.id:
                await delete_memory_by_id(operation.id, user=user)
                await add_memory(
                    request=request,
                    form_data=AddMemoryForm(content=operation.content),
                    user=user,
                )
                logger.info(f"UPDATE memory {operation.id}: {operation.content[:50]}...")

            elif operation.operation == "DELETE" and operation.id:
                await delete_memory_by_id(operation.id, user=user)
                logger.info(f"DELETE memory {operation.id}")

            return True
        except Exception as e:
            logger.error(
                f"Error executing {operation.operation} for ID {operation.id}: {e}\n{traceback.format_exc()}"
            )
            return False

    async def _query_llm(self, system_prompt: str, user_prompt: str) -> str:
        """Query the OpenAI API with improved error handling."""
        session = None
        try:
            session = await self._get_aiohttp_session()
            
            url = f"{self.valves.api_url.rstrip('/')}/chat/completions"
            headers = {"Content-Type": "application/json"}
            
            if self.valves.api_key:
                headers["Authorization"] = f"Bearer {self.valves.api_key}"
                    
            payload = {
                "model": self.valves.model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                "temperature": 0.2,
                "top_p": 0.8,
                "max_tokens": 4096,
                "response_format": {"type": "json_object"}
            }

            async with session.post(url, json=payload, headers=headers) as response:
                response.raise_for_status()
                data = await response.json()
                
                if "choices" in data and len(data["choices"]) > 0:
                    return data["choices"][0]["message"]["content"]
                return f"Error: Unexpected OpenAI response format: {data}"
                
        except asyncio.TimeoutError:
            return "Error: OpenAI API request timed out."
        except aiohttp.ClientError as e:
            if session and session.closed:
                self._aiohttp_session = None
            return f"Error: OpenAI API connection error: {e}"
        except aiohttp.ServerTimeoutError:
            return "Error: OpenAI server timeout."
        except Exception as e:
            return f"Error: OpenAI API query failed: {e}"

    async def cleanup(self):
        """Clean up resources on shutdown."""
        logger.info("Cleaning up Neural Recall resources...")
        
        await self._ensure_session_closed()
        
        if self._sentence_model is not None:
            try:
                del self._sentence_model
                self._sentence_model = None
                logger.info("Sentence transformer model cleaned up")
            except Exception as e:
                logger.warning(f"Error cleaning up sentence transformer model: {e}")
        
        logger.info("Cleanup complete.")
