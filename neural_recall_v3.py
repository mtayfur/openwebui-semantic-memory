"""
title: Neural Recall v3 - Dual Pipeline Architecture
version: 3.0.0

Complete architectural refactor implementing Retrieval Pipeline / Consolidation Pipeline model:
- Retrieval Pipeline: Synchronous retrieval and re-ranking in inlet for context injection
- Consolidation Pipeline: Asynchronous consolidation in outlet for memory management
"""

import asyncio
import hashlib
import json
import logging
import re
from collections import OrderedDict
from datetime import datetime, timezone
from typing import Any, Awaitable, Callable, Dict, List, Literal, Optional, Tuple, Union

import aiohttp
import numpy as np
from pydantic import BaseModel, Field, ValidationError as PydanticValidationError
from sentence_transformers import SentenceTransformer

from open_webui.models.users import Users
from open_webui.routers.memories import Memories

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

logger = logging.getLogger("NeuralRecallV3")
logger.setLevel(logging.INFO)


class NeuralRecallError(Exception):
    """Base exception for Neural Recall operations."""
    pass


class ModelLoadError(NeuralRecallError):
    """Raised when sentence transformer model fails to load."""
    pass


class EmbeddingError(NeuralRecallError):
    """Raised when embedding generation fails."""
    pass


class MemoryOperationError(NeuralRecallError):
    """Raised when memory operations fail."""
    pass


class ValidationError(NeuralRecallError):
    """Raised when data validation fails."""
    pass


class Config:
    """Centralized configuration constants for Neural Recall system."""

    CACHE_MAX_SIZE = 2500
    MAX_USER_CACHES = 500

    MAX_MESSAGE_LENGTH = 3000
    MAX_MEMORY_CONTENT_LENGTH = 500
    MIN_QUERY_LENGTH = 10

    TIMEOUT_SESSION_REQUEST = 30
    TIMEOUT_DATABASE_OPERATION = 10
    TIMEOUT_USER_LOOKUP = 5

    DEFAULT_SEMANTIC_THRESHOLD = 0.50
    
    DEFAULT_MAX_MEMORIES_RETURNED = 15 
    MIN_BATCH_SIZE = 8
    MAX_BATCH_SIZE = 32

    RETRIEVAL_MULTIPLIER = 3.0
    RETRIEVAL_TIMEOUT = 5.0
    
    CONSOLIDATION_CANDIDATE_SIZE = 50
    CONSOLIDATION_TIMEOUT = 30.0
    CONSOLIDATION_RELAXED_MULTIPLIER = 0.9

    STATUS_MESSAGES = {
        'SKIP_EMPTY': 'empty message',
        'SKIP_TOO_LONG': 'message too long',
        'SKIP_CODE': 'detected code content',
        'SKIP_STRUCTURED': 'detected structured data',
        'SKIP_SYMBOLS': 'message mostly symbols/numbers',
        'SKIP_LOGS': 'detected log dump',
        'SKIP_STACKTRACE': 'detected stack trace',
        'SKIP_URL_DUMP': 'detected URL dump',
    }


MEMORY_RERANKING_PROMPT = f"""You are the Retrieval Pipeline memory re-ranker. Rapidly select focused, atomic memories that will materially improve the AI's next reply.

Priority memory types (seek these first):
- Project status, deadlines, and current work context
- Skills, expertise levels, and tools the user employs
- Personal preferences that affect recommendations (dietary, accessibility, work style)
- Ongoing challenges, goals, or learning objectives
- Relationships, roles, and professional context
- Recent events or changes in circumstances
- Location, time zone, or environmental context that impacts suggestions

Ranking rules (apply in order):
1) **Direct relevance**: prefer memories that directly answer or inform the user's question.
2) **Context boost**: prefer memories that supply missing background (preferences, recent events, open tasks).
3) **Atomic focus**: favor single-topic memories over complex multi-fact entries that contain irrelevant information.
4) **Temporal relevance**: prefer recent facts when they change the answer or context significantly.
5) **Completeness**: ensure selected memories form coherent context without gaps.
6) **Quality over quantity**: exclude vague personal trivia or marginally related facts.

Output contract:
- Return ONLY a JSON array of memory IDs (strings) ordered by relevance, highest first.
- If none materially improve the response, return an empty array.

Examples (LLM must output only the JSON arrays shown):

1) Project status inquiry ("How's my website redesign going?")
Candidates: ["mem-001: User is working on website redesign for TechCorp with deadline August 22 2025", "mem-002: User prefers blue color schemes in UI design", "mem-003: User's cat is named Whiskers", "mem-004: User completed React basics course at FreeCodeCamp", "mem-005: User has design team meeting Friday and completed wireframes", "mem-006: User uses VS Code as primary development editor", "mem-007: User struggles with CSS grid layouts"]
Expected output:
["mem-001","mem-005","mem-007","mem-004"]

2) Cooking advice request ("What should I make for dinner tonight?")
Candidates: ["mem-010: User is vegetarian with severe nut allergy, dislikes spicy food", "mem-011: User enjoys Italian cuisine and prefers quick weeknight meals", "mem-012: User works remotely from home office in Seattle", "mem-013: User bought fresh tomatoes and basil yesterday", "mem-014: User has pasta-making tools and prefers homemade meals", "mem-015: User dislikes cleaning up complex recipes on weekdays"]
Expected output:
["mem-010","mem-011","mem-015","mem-013","mem-014"]

3) Technical help with Python data analysis
Candidates: ["mem-020: User's favorite movie is Inception", "mem-021: User works as data analyst at TechCorp", "mem-022: User completed Python basics course and now working on pandas projects", "mem-023: User struggles with matplotlib visualization despite being comfortable with pandas", "mem-024: User prefers Jupyter notebooks for data exploration", "mem-025: User has deadline for quarterly report analysis August 25 2025"]
Expected output:
["mem-021","mem-022","mem-023","mem-024","mem-025"]

4) General conversation with no specific memory relevance
Candidates: ["mem-030: User's favorite movie is Inception", "mem-031: User works remotely from Seattle", "mem-032: User owns a cat named Whiskers"]
Expected output:
[]

5) Travel planning inquiry ("I need help planning my trip to Japan")
Candidates: ["mem-040: User is vegetarian with severe nut allergy", "mem-041: User speaks conversational Japanese from college courses", "mem-042: User prefers budget-friendly travel options", "mem-043: User has visited Tokyo before in 2022", "mem-044: User works as software engineer", "mem-045: User enjoys temples, gardens, and traditional architecture", "mem-046: User is planning Japan trip for December 2025"]
Expected output:
["mem-046","mem-041","mem-043","mem-045","mem-042","mem-040"]

CRITICAL: Response must be valid JSON (only the array). No commentary, no code fences, no extra fields."""


MEMORY_CONSOLIDATION_PROMPT = f"""You are the Consolidation Pipeline memory consolidator. STRICTLY enforce merging/splitting when clearly required. AVOID unnecessary trivial updates.

MANDATORY OPERATIONS (STRICTLY ENFORCE):

**REQUIRED MERGES** - MUST merge when:
- Fragmented facts about same entity (person, project, skill)
- Scattered constraints (dietary, accessibility, preferences)
- Project details spread across memories (name, status, deadlines)
- Learning stages for same technology/domain
- Related location/workplace/context details

**REQUIRED SPLITS** - MUST split when:
- Topic contamination (work + personal + random facts mixed)
- Temporal conflicts (different time periods bundled incorrectly)
- Functional separation needed (unrelated skills/tools mixed)
- Information overload (too many disparate facts)

**REQUIRED DELETIONS** - MUST delete when:
- Newer dated information supersedes older conflicting facts
- Exact/near-identical duplicates exist
- Clearly obsolete information

**NO-OPERATION CRITERIA** - Return [] ONLY when:
- Memories already optimally organized (atomic but not fragmented)
- No clear merging/splitting/deletion opportunities exist
- Changes would be purely cosmetic

Core principles:
1. **Verifiable Factuality**: Record only what is explicitly stated
2. **Informational Density**: Group related details into dense memories
3. **Temporal Precedence**: New dated information supersedes older conflicting data
4. **Contextual Grounding**: Convert relative references to absolute dates using Current Date/Time
5. **Rich Context**: Include names, pronouns for searchability

Content constraints:
- Max length: {Config.MAX_MEMORY_CONTENT_LENGTH} characters
- Start with "User" or "User's"
- Use Month Day Year format (e.g., "December 12 2024")
- Include specific names, dates, context
- Ensure self-contained and actionable

Output: JSON array only. Operations: CREATE (content), UPDATE (id+content), DELETE (id).

EXAMPLES:

A) MERGE - Fragmented project:
Input: ["mem-101: User mentioned TechCorp project", "mem-102: User has deadline next Friday", "mem-103: User is working on mobile app redesign"]
Output: [{{"operation":"UPDATE","id":"mem-101","content":"User is working on mobile app redesign project for TechCorp with deadline August 22 2025"}},{{"operation":"DELETE","id":"mem-102"}},{{"operation":"DELETE","id":"mem-103"}}]

B) SPLIT - Topic contamination:
Input: ["mem-201: User works as Senior Software Engineer at TechCorp (promoted March 2024), leads mobile app team, uses React and Python, prefers VS Code editor, has design meeting Friday, completed wireframes, likes coffee, owns a cat named Whiskers"]
Output: [{{"operation":"UPDATE","id":"mem-201","content":"User is Senior Software Engineer at TechCorp (promoted March 2024) leading mobile app team"}},{{"operation":"CREATE","content":"User uses React and Python for development work with VS Code as preferred editor"}},{{"operation":"CREATE","content":"User has design team meeting Friday and completed wireframes for current project"}},{{"operation":"CREATE","content":"User owns a cat named Whiskers"}}]

C) MERGE - Scattered constraints:
Input: ["mem-301: User is vegetarian", "mem-302: User has severe nut allergy", "mem-303: User dislikes spicy food", "mem-304: User enjoys Italian cuisine"]
Output: [{{"operation":"UPDATE","id":"mem-301","content":"User is vegetarian with severe nut allergy, dislikes spicy food, enjoys Italian cuisine and prefers quick weeknight meals"}},{{"operation":"DELETE","id":"mem-302"}},{{"operation":"DELETE","id":"mem-303"}},{{"operation":"DELETE","id":"mem-304"}}]

D) DELETE - Temporal conflicts:
Input: ["mem-401: User enjoys steak dinners", "mem-402: User became fully vegetarian December 2024", "mem-403: User stopped eating meat products"]
Output: [{{"operation":"UPDATE","id":"mem-402","content":"User became fully vegetarian December 2024 (no longer eats meat products)"}},{{"operation":"DELETE","id":"mem-401"}},{{"operation":"DELETE","id":"mem-403"}}]

E) MERGE - Skill progression:
Input: ["mem-501: User learning Python", "mem-502: User completed Python basics course at DataCamp", "mem-503: User now working on data analysis with pandas library"]
Output: [{{"operation":"UPDATE","id":"mem-502","content":"User completed Python basics course at DataCamp and now working on data analysis with pandas library"}},{{"operation":"DELETE","id":"mem-501"}},{{"operation":"DELETE","id":"mem-503"}}]

F) NO ACTION - Already optimized:
Input: ["mem-601: User works as Senior Data Analyst at Microsoft with 5 years experience", "mem-602: User lives in Seattle with wife Sarah and two children", "mem-603: User trains for marathon running and follows vegetarian diet"]
Output: []

CRITICAL: Only act when clear organizational benefit exists. Return valid JSON array only."""


class MemoryOperation(BaseModel):
    """Pydantic model for memory operations with validation."""
    
    operation: Literal["CREATE", "UPDATE", "DELETE"]
    content: Optional[str] = None
    id: Optional[str] = None

    def validate_operation(self, existing_memory_ids: set = None) -> bool:
        """Validate the memory operation according to business rules."""
        if existing_memory_ids is None:
            existing_memory_ids = set()
            
        if self.operation == "CREATE":
            return self.content is not None and len(self.content.strip()) > 0
        elif self.operation in ["UPDATE", "DELETE"]:
            if not self.id or self.id not in existing_memory_ids:
                return False
            if self.operation == "UPDATE":
                return self.content is not None and len(self.content.strip()) > 0
            return True
        return False


class LRUCache:
    """Enhanced thread-safe LRU cache implementation using OrderedDict with comprehensive metrics."""

    def __init__(self, max_size: int) -> None:
        """Initialize LRU cache with specified maximum size."""
        self.max_size = max_size
        self._cache: OrderedDict[str, Any] = OrderedDict()
        self._lock = asyncio.Lock()

        self._hits = 0
        self._misses = 0
        self._evictions = 0

    async def get(self, key: str) -> Optional[Any]:
        """Get an item from cache, moving it to end (most recently used)."""
        async with self._lock:
            if key in self._cache:
                value = self._cache.pop(key)
                self._cache[key] = value
                self._hits += 1
                return value
            self._misses += 1
            return None

    async def put(self, key: str, value: Any) -> None:
        """Put an item in cache, evicting least recently used if necessary."""
        async with self._lock:
            if key in self._cache:
                self._cache.pop(key)
            elif len(self._cache) >= self.max_size:
                self._cache.popitem(last=False)
                self._evictions += 1
            self._cache[key] = value

    async def size(self) -> int:
        """Get current cache size."""
        async with self._lock:
            return len(self._cache)

    async def clear(self) -> int:
        """Clear cache and return number of entries cleared."""
        async with self._lock:
            size = len(self._cache)
            self._cache.clear()
            self._reset_metrics()
            return size

    async def contains(self, key: str) -> bool:
        """Check if key exists in cache without updating access order."""
        async with self._lock:
            return key in self._cache

    async def get_stats(self) -> Dict[str, Union[int, float]]:
        """Get cache statistics."""
        async with self._lock:
            total_requests = self._hits + self._misses
            hit_rate = (self._hits / total_requests * 100) if total_requests > 0 else 0.0
            return {
                "size": len(self._cache),
                "hits": self._hits,
                "misses": self._misses,
                "evictions": self._evictions,
                "hit_rate": hit_rate,
            }

    def _reset_metrics(self) -> None:
        """Reset cache metrics."""
        self._hits = 0
        self._misses = 0
        self._evictions = 0


class Filter:
    """Enhanced multi-model embedding and memory filter with LRU caching."""

    _model = None
    _model_load_lock = None
    _embedding_cache: Dict[str, LRUCache] = {}
    _cache_access_order: List[str] = []
    _cache_lock = None
    _aiohttp_session: Optional[aiohttp.ClientSession] = None
    _session_lock = None

    class Valves(BaseModel):
        """Configuration valves for the Neural Recall system."""
        
        api_url: str = Field(
            default="http://host.docker.internal:11434/v1",
            description="OpenAI-compatible API URL for LLM operations",
        )
        api_key: str = Field(
            default="sk-neural-recall-key",
            description="API key for LLM service authentication",
        )
        model: str = Field(
            default="llama3.2:3b",
            description="Model name for LLM operations",
        )
        
        embedding_model: str = Field(
            default="Alibaba-NLP/gte-multilingual-base",
            description="Sentence transformer model for embeddings",
        )
        
        max_memories_returned: int = Field(
            default=Config.DEFAULT_MAX_MEMORIES_RETURNED,
            description="Maximum number of memories to inject into context",
        )
        semantic_threshold: float = Field(
            default=Config.DEFAULT_SEMANTIC_THRESHOLD,
            description="Minimum similarity threshold for memory retrieval",
        )


    def __init__(self):
        """Initialize the Neural Recall filter with production validation."""
        self.valves = self.Valves()    
        self._validate_configuration()
    
    def _validate_configuration(self) -> None:
        """Validate configuration and fail if invalid."""
        if not self.valves.api_url or not self.valves.api_url.strip():
            raise ValidationError("API URL not configured")
        
        if not self.valves.model or not self.valves.model.strip():
            raise ValidationError("Model not specified")
        
        if not 0.0 <= self.valves.semantic_threshold <= 1.0:
            raise ValidationError(f"Invalid semantic threshold: {self.valves.semantic_threshold}")
            
        if self.valves.max_memories_returned <= 0:
            raise ValidationError(f"Invalid max memories returned: {self.valves.max_memories_returned}")
            
        logger.info("âœ… Configuration validated")

    async def _get_embedding_model(self):
        """Get or load the sentence transformer model with thread safety."""
        if Filter._model is None:
            if Filter._model_load_lock is None:
                Filter._model_load_lock = asyncio.Lock()
            
            async with Filter._model_load_lock:
                if Filter._model is None:
                    try:
                        logger.info(f"ðŸ¤– Loading embedding model: {self.valves.embedding_model}")
                        
                        def load_model():
                            return SentenceTransformer(
                                self.valves.embedding_model,
                                device="cpu",
                                trust_remote_code=True,
                            )
                        
                        loop = asyncio.get_event_loop()
                        Filter._model = await loop.run_in_executor(None, load_model)
                        logger.info("âœ… Embedding model loaded successfully")
                        
                    except Exception as e:
                        error_msg = f"Failed to load embedding model: {str(e)}"
                        logger.error(f"âŒ {error_msg}")
                        raise ModelLoadError(error_msg)
        
        return Filter._model

    async def _get_user_cache(self, user_id: str) -> LRUCache:
        """Get or create user-specific embedding cache with global user limit."""
        if Filter._cache_lock is None:
            Filter._cache_lock = asyncio.Lock()
            
        async with Filter._cache_lock:
            if user_id in Filter._embedding_cache:
                if user_id in Filter._cache_access_order:
                    Filter._cache_access_order.remove(user_id)
                Filter._cache_access_order.append(user_id)
                return Filter._embedding_cache[user_id]
            
            if len(Filter._embedding_cache) >= Config.MAX_USER_CACHES:
                if Filter._cache_access_order:
                    lru_user_id = Filter._cache_access_order.pop(0)
                    if lru_user_id in Filter._embedding_cache:
                        del Filter._embedding_cache[lru_user_id]
                        logger.info(f"ðŸ§¹ Cache evicted for user {lru_user_id}")
            
            Filter._embedding_cache[user_id] = LRUCache(Config.CACHE_MAX_SIZE)
            Filter._cache_access_order.append(user_id)
            return Filter._embedding_cache[user_id]

    async def _generate_embedding(self, text: str, user_id: str) -> np.ndarray:
        """Generate embedding for text with caching support."""
        if not text or len(text.strip()) < Config.MIN_QUERY_LENGTH:
            raise EmbeddingError("Text too short for embedding generation")

        text_hash = hashlib.sha256(text.encode()).hexdigest()
        cache = await self._get_user_cache(user_id)
        
        cached_embedding = await cache.get(text_hash)
        if cached_embedding is not None:
            return cached_embedding

        try:
            model = await self._get_embedding_model()
            
            def generate_embedding():
                embedding = model.encode([text], convert_to_numpy=True)[0]
                norm = np.linalg.norm(embedding)
                if norm > 0:
                    embedding = embedding / norm
                return embedding
            
            loop = asyncio.get_event_loop()
            embedding = await loop.run_in_executor(None, generate_embedding)
            
            await cache.put(text_hash, embedding)
            return embedding
            
        except Exception as e:
            error_msg = f"Embedding generation failed: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            raise EmbeddingError(error_msg)

    async def _generate_embeddings_batch(self, texts: List[str], user_id: str) -> List[np.ndarray]:
        """Generate embeddings for multiple texts efficiently with batch processing and caching."""
        if not texts:
            return []
        
        cache = await self._get_user_cache(user_id)
        
        cached_embeddings = {}
        uncached_texts = []
        uncached_indices = []
        
        for i, text in enumerate(texts):
            if not text or len(text.strip()) < Config.MIN_QUERY_LENGTH:
                continue
                
            text_hash = hashlib.sha256(text.encode()).hexdigest()
            cached_embedding = await cache.get(text_hash)
            
            if cached_embedding is not None:
                cached_embeddings[i] = cached_embedding
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)
        
        new_embeddings = {}
        if uncached_texts:
            try:
                model = await self._get_embedding_model()
                
                batch_size = min(Config.MAX_BATCH_SIZE, max(Config.MIN_BATCH_SIZE, len(uncached_texts)))
                
                def generate_batch_embeddings(batch_texts):
                    embeddings = model.encode(batch_texts, convert_to_numpy=True, show_progress_bar=False)
                    normalized_embeddings = []
                    for embedding in embeddings:
                        norm = np.linalg.norm(embedding)
                        if norm > 0:
                            embedding = embedding / norm
                        normalized_embeddings.append(embedding)
                    return normalized_embeddings
                
                loop = asyncio.get_event_loop()
                
                for i in range(0, len(uncached_texts), batch_size):
                    batch_texts = uncached_texts[i:i + batch_size]
                    batch_indices = uncached_indices[i:i + batch_size]
                    
                    batch_embeddings = await loop.run_in_executor(
                        None, generate_batch_embeddings, batch_texts
                    )
                    
                    for j, embedding in enumerate(batch_embeddings):
                        text_idx = batch_indices[j]
                        text = batch_texts[j]
                        text_hash = hashlib.sha256(text.encode()).hexdigest()
                        
                        await cache.put(text_hash, embedding)
                        new_embeddings[text_idx] = embedding
                        
            except Exception as e:
                error_msg = f"Batch embedding generation failed: {str(e)}"
                logger.error(f"âŒ {error_msg}")
                raise EmbeddingError(error_msg)
        
        result_embeddings = []
        for i in range(len(texts)):
            if i in cached_embeddings:
                result_embeddings.append(cached_embeddings[i])
            elif i in new_embeddings:
                result_embeddings.append(new_embeddings[i])
            else:
                result_embeddings.append(None)
        
        result_embeddings = [emb for emb in result_embeddings if emb is not None]
        
        logger.info(f"ðŸš€ Batch embedding: {len(cached_embeddings)} cached, {len(new_embeddings)} new, {len(result_embeddings)} total")
        return result_embeddings

    async def _get_aiohttp_session(self) -> aiohttp.ClientSession:
        """Get or create aiohttp session with connection pooling and production settings."""
        if Filter._aiohttp_session is None or Filter._aiohttp_session.closed:
            if Filter._session_lock is None:
                Filter._session_lock = asyncio.Lock()
                
            async with Filter._session_lock:
                if Filter._aiohttp_session is None or Filter._aiohttp_session.closed:
                    timeout = aiohttp.ClientTimeout(total=Config.TIMEOUT_SESSION_REQUEST)
                    connector = aiohttp.TCPConnector(
                        limit=100, 
                        limit_per_host=30,
                        keepalive_timeout=300,
                        enable_cleanup_closed=True,
                        ttl_dns_cache=300,
                    )
                    Filter._aiohttp_session = aiohttp.ClientSession(
                        timeout=timeout, 
                        connector=connector,
                        headers={"User-Agent": "NeuralRecallV3/3.0.0"}
                    )
        return Filter._aiohttp_session

    def _should_skip_memory_operations(self, user_message: str) -> Tuple[bool, str]:
        """
        Enhanced gating for memory operations with comprehensive pattern detection.
        Skips: empty, too long, code, logs, structured data, URL dumps, or symbol spam.
        """
        if not user_message or not user_message.strip():
            return True, Config.STATUS_MESSAGES['SKIP_EMPTY']

        trimmed_message = user_message.strip()

        if len(trimmed_message) > Config.MAX_MESSAGE_LENGTH:
            return True, f"{Config.STATUS_MESSAGES['SKIP_TOO_LONG']} ({len(trimmed_message)} chars > {Config.MAX_MESSAGE_LENGTH})"

        code_patterns = [
            r'```[\s\S]*?```',        
            r'`[^`\n]+`',                  
            r'^\s*(def|class|var|let|const)\s+[\w_][\w\d_]*\s*[=\(\{;:]',
            r'^\s*(import|from)\s+[\w\.\-_]+',       
            r'^\s*function\s+\w+\s*\(',         
            r'if\s*\(.*\)\s*\{',         
            r'for\s*\(.*\)\s*\{',            
            r'while\s*\(.*\)\s*\{',           
            r'#include\s*<',                 
            r'using\s+namespace',              
            r'SELECT\s+.+\s+FROM\s+\w+',         
            r'INSERT\s+INTO\s+\w+',             
            r'UPDATE\s+\w+\s+SET\s+\w+\s*=',    
            r'DELETE\s+FROM\s+\w+',           
            r'^\s*[A-Za-z_]\w*\s*[=:]\s*[A-Za-z_]\w*\s*\(',
            r'\w+\s*=\s*\([^)]*\)\s*=>\s*\{',  
            r'^\s*\{\s*\}\s*$',                 
            r'<[^>]+>[\s\S]*</[^>]+>',        
        ]
        
        for pattern in code_patterns:
            if re.search(pattern, trimmed_message, re.MULTILINE | re.IGNORECASE):
                return True, Config.STATUS_MESSAGES['SKIP_CODE']
        
        message_lines = trimmed_message.split('\n')
        line_count = len(message_lines)
        
        count_lines = lambda pattern: sum(1 for line in message_lines if re.match(pattern, line))
        
        if ((line_count >= 3 and count_lines(r'^\s*[A-Za-z0-9_]+:\s+\S+') >= max(3, line_count * 0.6)) or
            (line_count >= 4 and count_lines(r'^\s*[-\*\+]\s+.+') >= max(4, line_count * 0.6)) or
            re.search(r'^\s*[\{\[].*[\}\]]\s*$', trimmed_message, re.DOTALL)):
            return True, Config.STATUS_MESSAGES['SKIP_STRUCTURED']
        
        if len(trimmed_message) > 10 and sum(1 for c in trimmed_message if c.isalpha() or c.isspace()) / len(trimmed_message) < 0.5:
            return True, Config.STATUS_MESSAGES['SKIP_SYMBOLS']
        
        if (line_count >= 3 and 
            (sum(1 for line in message_lines if '|' in line and line.count('|') >= 2) >= max(2, line_count * 0.6) or
             count_lines(r'^\s*\d+\.\s') >= max(3, line_count * 0.6) or
             count_lines(r'^\s*[a-zA-Z]\)\s') >= max(3, line_count * 0.6))):
            return True, Config.STATUS_MESSAGES['SKIP_STRUCTURED']
        
        if line_count >= 2:
            if sum(1 for line in message_lines if re.search(r'\d{2,4}[-/]\d{1,2}[-/]\d{1,2}.*\d{1,2}:\d{2}', line)) >= max(1, line_count * 0.3):
                return True, Config.STATUS_MESSAGES['SKIP_LOGS']
            
            stack_patterns = [
                r'^\s*(at|File|Traceback|Exception|Error).*:\d+',
                r'Traceback\s*\(most recent call',
                r'^\s*File\s*"[^"]+",\s*line\s*\d+',
                r'^\s*at\s+\w+.*\([^)]*:\d+:\d+\)',
                r'Exception\s+in\s+thread',
                r'^\s*(NameError|TypeError|ValueError|AttributeError|KeyError):'
            ]
            if any(any(re.search(pattern, line) for pattern in stack_patterns) for line in message_lines):
                return True, Config.STATUS_MESSAGES['SKIP_STACKTRACE']
        
        single_line_error_patterns = [
            r'(NameError|TypeError|ValueError|AttributeError|KeyError|SyntaxError|IndentationError).*:',
            r'\d{2,4}[-/]\d{1,2}[-/]\d{1,2}[\s\d:]+\b(ERROR|WARN|INFO|DEBUG)\b',
            r'\bERROR\b.*:.*',
            r'^\s*(at|File)\s+["\']?[^"\']*["\']?,?\s*line\s+\d+',
            r'^\s*at\s+\w+.*\([^)]*:\d+:\d+\)',
            r'Exception\s+in\s+thread',
        ]
        
        for pattern in single_line_error_patterns:
            if re.search(pattern, trimmed_message, re.IGNORECASE):
                return True, Config.STATUS_MESSAGES['SKIP_LOGS']
        
        if len(re.findall(r'https?://[^\s]+', trimmed_message)) >= 3:
            return True, Config.STATUS_MESSAGES['SKIP_URL_DUMP']
        
        if len(trimmed_message) < Config.MIN_QUERY_LENGTH:
            return True, Config.STATUS_MESSAGES['SKIP_EMPTY']

        return False, ""

    def _extract_text_from_message_content(self, content: Union[str, List[Dict], Dict]) -> str:
        """Extract text from various message content formats."""
        if isinstance(content, str):
            return content
        elif isinstance(content, list):
            text_parts = []
            for item in content:
                if isinstance(item, dict) and item.get("type") == "text":
                    text_parts.append(item.get("text", ""))
            return " ".join(text_parts)
        elif isinstance(content, dict) and "text" in content:
            return content["text"]
        return ""

    def _inject_datetime_context(self, body: Dict[str, Any]) -> None:
        """Inject current datetime context into the conversation."""
        datetime_string = self.get_formatted_datetime_string()
        system_message = {
            "role": "system",
            "content": f"Current Date/Time: {datetime_string}",
        }

        if "messages" in body:
            existing_system_messages = [
                msg for msg in body["messages"] if msg.get("role") == "system"
            ]
            if not existing_system_messages:
                body["messages"].insert(0, system_message)
            else:
                for msg in existing_system_messages:
                    if "Current Date/Time:" not in msg.get("content", ""):
                        msg["content"] += f"\n{system_message['content']}"
                        break
                else:
                    body["messages"].insert(0, system_message)

    def get_formatted_datetime_string(self) -> str:
        """Get formatted datetime string for context injection."""
        try:
            now = datetime.now(timezone.utc)
            weekday = now.strftime("%A")
            month = now.strftime("%B")
            day = now.day
            year = now.year
            time_str = now.strftime("%H:%M:%S UTC")
            return f"{weekday} {month} {day} {year} at {time_str}"
        except Exception as e:
            raise NeuralRecallError(f"Failed to format datetime: {e}")

    async def _emit_status(
        self,
        emitter: Optional[Callable[[Any], Awaitable[None]]],
        message: str,
        done: bool = False,
    ) -> None:
        """Emit status message through the event emitter."""
        if emitter:
            await emitter(
                {
                    "type": "status",
                    "data": {"description": message, "done": done},
                }
            )

    async def _broad_retrieval(self, user_message: str, user_id: str, emitter: Optional[Callable[[Any], Awaitable[None]]] = None) -> List[Dict[str, Any]]:
        """Step 1 of Retrieval Pipeline: Perform broad vector search for candidate memories."""
        candidate_limit = int(self.valves.max_memories_returned * Config.RETRIEVAL_MULTIPLIER)
        
        user_memories = await asyncio.wait_for(
            asyncio.to_thread(Memories.get_memories_by_user_id, user_id),
            timeout=Config.RETRIEVAL_TIMEOUT
        )
        
        if not user_memories:
            logger.info("ðŸ” Retrieval Pipeline: No memories found for user")
            return []
        
        logger.info(f"ðŸ” Retrieval Pipeline: Retrieved {len(user_memories)} total memories")
        
        query_embedding = await self._generate_embedding(user_message, user_id)
        
        memory_contents = [memory.content for memory in user_memories]
        
        memory_embeddings = await self._generate_embeddings_batch(memory_contents, user_id)
        
        if len(memory_embeddings) != len(user_memories):
            error_msg = f"Embedding count mismatch: {len(memory_embeddings)} vs {len(user_memories)}"
            logger.error(f"âŒ {error_msg}")
            raise EmbeddingError(error_msg)
        
        formatted_memories = []
        
        for i, memory in enumerate(user_memories):
            memory_embedding = memory_embeddings[i]
            similarity = float(np.dot(query_embedding, memory_embedding))
            
            if similarity >= self.valves.semantic_threshold :
                memory_dict = {
                    "id": str(memory.id),
                    "content": memory.content,
                    "relevance": similarity,
                }
                formatted_memories.append(memory_dict)
        
        formatted_memories.sort(key=lambda x: x["relevance"], reverse=True)
        formatted_memories = formatted_memories[:candidate_limit]
        
        logger.info(f"ðŸ” Retrieval Pipeline: {len(formatted_memories)} candidates from {len(user_memories)} total memories (threshold: {self.valves.semantic_threshold :.3f})")
        
        if len(formatted_memories) >= candidate_limit and len(user_memories) > candidate_limit:
            await self._emit_status(emitter, f"ðŸ“š Found {len(user_memories)} memories, selected {len(formatted_memories)} candidates", False)
        else:
            await self._emit_status(emitter, f"ðŸ“š Found {len(formatted_memories)} memory candidates", False)
        
        return formatted_memories

    async def _llm_rerank_memories(self, user_message: str, candidate_memories: List[Dict[str, Any]], emitter: Optional[Callable[[Any], Awaitable[None]]] = None) -> List[Dict[str, Any]]:
        """Step 2 of Retrieval Pipeline: Use LLM to re-rank candidate memories."""
        if not candidate_memories:
            return []
        
        memory_context = "## CANDIDATE MEMORIES\n"
        for memory in candidate_memories:
            memory_context += f"ID: {memory['id']}\nContent: {memory['content']}\n\n"
        
        system_prompt = MEMORY_RERANKING_PROMPT + f"\n\n{memory_context}"
        
        await self._emit_status(emitter, "ðŸ¤– LLM re-ranking memories...", False)
        response = await asyncio.wait_for(
            self._query_llm(system_prompt, user_message, json_response=True),
            timeout=Config.RETRIEVAL_TIMEOUT
        )
        
        if not response or response.startswith("Error:"):
            error_msg = "LLM re-ranking failed - empty or error response"
            logger.error(f"âŒ {error_msg}")
            raise NeuralRecallError(error_msg)
        
        ranked_ids = self._extract_and_parse_json(response)
        if not isinstance(ranked_ids, list):
            error_msg = "Invalid LLM re-ranking response - expected list"
            logger.error(f"âŒ {error_msg}")
            raise NeuralRecallError(error_msg)
        
        memory_lookup = {mem['id']: mem for mem in candidate_memories}
        ranked_memories = []
        
        for memory_id in ranked_ids:
            if memory_id in memory_lookup:
                ranked_memories.append(memory_lookup[memory_id])
        
        ranked_ids_set = set(ranked_ids)
        for memory in candidate_memories:
            if memory['id'] not in ranked_ids_set:
                ranked_memories.append(memory)
        
        final_memories = ranked_memories[:self.valves.max_memories_returned]
        
        logger.info(f"ðŸŽ¯ Retrieval Pipeline: {len(final_memories)} memories selected from {len(candidate_memories)} candidates after LLM re-ranking")
        
        if len(final_memories) >= self.valves.max_memories_returned and len(ranked_memories) > self.valves.max_memories_returned:
            await self._emit_status(emitter, f"ðŸŽ¯ Ranked {len(ranked_memories)} memories, selected {len(final_memories)}", False)
        else:
            await self._emit_status(emitter, f"ðŸŽ¯ Selected {len(final_memories)} memories", False)
        
        return final_memories

    def _inject_memories_into_context(self, body: Dict[str, Any], memories: List[Dict[str, Any]], emitter: Optional[Callable[[Any], Awaitable[None]]] = None) -> None:
        """Inject selected memories into the conversation context."""
        if not memories:
            return
        memory_header = f"## RETRIEVED MEMORIES\n"
        memory_content = "\n".join([f"- {memory['content']}" for memory in memories])
        memory_injection = f"{memory_header}{memory_content}\n"
        
        if "messages" in body and body["messages"]:
            for i in range(len(body["messages"]) - 1, -1, -1):
                if body["messages"][i].get("role") == "user":
                    memory_message = {
                        "role": "system",
                        "content": memory_injection
                    }
                    body["messages"].insert(i, memory_message)
                    logger.info(f"ðŸ’¡ Injected {len(memories)} memories into conversation context")
                    break

    async def _gather_consolidation_candidates(self, user_message: str, user_id: str, emitter: Optional[Callable[[Any], Awaitable[None]]] = None) -> List[Dict[str, Any]]:
        """Step 1 of Consolidation Pipeline: Gather candidate memories for consolidation analysis."""
        user_memories = await asyncio.wait_for(
            asyncio.to_thread(Memories.get_memories_by_user_id, user_id),
            timeout=Config.CONSOLIDATION_TIMEOUT
        )
        
        if not user_memories:
            logger.info("ðŸ“Š Consolidation Pipeline: No memories found for user")
            await self._emit_status(emitter, "ðŸ’­ No memories to analyze", True)
            return []

        logger.info(f"ðŸ“Š Consolidation Pipeline: Retrieved {len(user_memories)} total memories for consolidation analysis")
        
        query_embedding = await self._generate_embedding(user_message, user_id)
        
        memory_contents = [memory.content for memory in user_memories]
        
        memory_embeddings = await self._generate_embeddings_batch(memory_contents, user_id)
        
        if len(memory_embeddings) != len(user_memories):
            error_msg = f"Consolidation embedding count mismatch: {len(memory_embeddings)} vs {len(user_memories)}"
            logger.error(f"âŒ {error_msg}")
            raise EmbeddingError(error_msg)
        
        formatted_memories = []
        relaxed_threshold = self.valves.semantic_threshold * Config.CONSOLIDATION_RELAXED_MULTIPLIER
        
        for i, memory in enumerate(user_memories):
            memory_embedding = memory_embeddings[i]
            similarity = float(np.dot(query_embedding, memory_embedding))
            
            if similarity >= relaxed_threshold:
                created_at_iso = None
                if hasattr(memory, 'created_at') and memory.created_at:
                    created_at_iso = datetime.fromtimestamp(memory.created_at, tz=timezone.utc).isoformat()
                
                updated_at_iso = None
                if hasattr(memory, 'updated_at') and memory.updated_at:
                    updated_at_iso = datetime.fromtimestamp(memory.updated_at, tz=timezone.utc).isoformat()
                
                memory_dict = {
                    "id": str(memory.id),
                    "content": memory.content,
                    "created_at": created_at_iso,
                    "updated_at": updated_at_iso,
                    "relevance": similarity,
                }
                formatted_memories.append(memory_dict)
        
        formatted_memories.sort(key=lambda x: x["relevance"], reverse=True)
        formatted_memories = formatted_memories[:Config.CONSOLIDATION_CANDIDATE_SIZE]
        
        logger.info(f"ðŸ“Š Consolidation Pipeline: {len(formatted_memories)} candidates from {len(user_memories)} total memories (threshold: {relaxed_threshold:.3f})")
        
        if len(formatted_memories) >= Config.CONSOLIDATION_CANDIDATE_SIZE and len(user_memories) > Config.CONSOLIDATION_CANDIDATE_SIZE:
            await self._emit_status(emitter, f"ðŸ“Š Analyzing {len(user_memories)} memories, selected {len(formatted_memories)} candidates", False)
        else:
            await self._emit_status(emitter, f"ðŸ“Š Analyzing {len(formatted_memories)} memory candidates", False)
        
        return formatted_memories

    async def _llm_consolidate_memories(self, user_message: str, conversation_context: str, candidate_memories: List[Dict[str, Any]], emitter: Optional[Callable[[Any], Awaitable[None]]] = None) -> List[Dict[str, Any]]:
        """Step 2 of Consolidation Pipeline: Use LLM to generate consolidation plan."""
        if not candidate_memories:
            return []
        
        memory_context = "## CANDIDATE MEMORIES FOR CONSOLIDATION\n"
        for memory in candidate_memories:
            memory_context += f"ID: {memory['id']}\nContent: {memory['content']}\n"
            if memory.get('created_at'):
                memory_context += f"Created: {memory['created_at']}\n"
            if memory.get('updated_at'):
                memory_context += f"Updated: {memory['updated_at']}\n"
            memory_context += "\n"
        
        context_section = f"## CONVERSATION CONTEXT\nUser Message: {user_message}\nFull Context: {conversation_context}\n\n"
        
        date_context = f"## CURRENT DATE/TIME\n{self.get_formatted_datetime_string()}\n\n"
        
        system_prompt = MEMORY_CONSOLIDATION_PROMPT + f"\n\n{date_context}{context_section}{memory_context}"
        
        await self._emit_status(emitter, "ðŸ¤– LLM analyzing memory consolidation...", False)
        response = await asyncio.wait_for(
            self._query_llm(system_prompt, "Analyze the memories and generate a consolidation plan.", json_response=True),
            timeout=Config.CONSOLIDATION_TIMEOUT
        )
        
        if not response or response.startswith("Error:"):
            error_msg = "LLM consolidation analysis failed - empty or error response"
            logger.error(f"âŒ {error_msg}")
            raise NeuralRecallError(error_msg)
        
        logger.info(f"ðŸ”§ LLM consolidation response received: {len(response)} characters")
        
        operations = self._extract_and_parse_json(response)
        if not isinstance(operations, list):
            if isinstance(operations, dict) and operations:
                operations = [operations]
                logger.info("ðŸ”§ Wrapped single operation in list")
            else:
                error_msg = f"Invalid LLM consolidation response - Expected list, got {type(operations)}"
                logger.error(f"âŒ {error_msg}")
                raise NeuralRecallError(error_msg)
        
        existing_memory_ids = {memory["id"] for memory in candidate_memories}
        valid_operations = []
        
        for operation in operations:
            if isinstance(operation, dict):
                memory_operation = MemoryOperation(**operation)
                if memory_operation.validate_operation(existing_memory_ids):
                    valid_operations.append(operation)
        
        logger.info(f"ðŸ”§ Consolidation Pipeline: {len(valid_operations)} operations planned")
        await self._emit_status(emitter, f"ðŸŽ¯ {len(valid_operations)} memory operations planned", False)
        return valid_operations

    async def _execute_consolidation_operations(self, operations: List[Dict[str, Any]], user_id: str, emitter: Optional[Callable[[Any], Awaitable[None]]] = None) -> None:
        """Step 3 of Consolidation Pipeline: Execute consolidation operations against database."""
        if not operations:
            return
        
        user = await asyncio.wait_for(
            asyncio.to_thread(Users.get_user_by_id, user_id),
            timeout=Config.TIMEOUT_USER_LOOKUP,
        )
        
        if not user:
            error_msg = f"User not found for consolidation: {user_id}"
            logger.error(f"âŒ {error_msg}")
            raise MemoryOperationError(error_msg)
        
        created_count = 0
        updated_count = 0
        deleted_count = 0
        
        for operation_data in operations:
            operation = MemoryOperation(**operation_data)
            result = await self._execute_single_operation(operation, user)
            if result == "CREATE":
                created_count += 1
            elif result == "UPDATE":
                updated_count += 1
            elif result == "DELETE":
                deleted_count += 1
            
            total_executed = created_count + updated_count + deleted_count
            logger.info(f"âœ… Consolidation Pipeline: Executed {total_executed}/{len(operations)} operations (Created: {created_count}, Updated: {updated_count}, Deleted: {deleted_count})")
            
            result_parts = []
            if created_count > 0:
                result_parts.append(f"âœ¨ Created {created_count}")
            if updated_count > 0:
                result_parts.append(f"ðŸ“ Updated {updated_count}")
            if deleted_count > 0:
                result_parts.append(f"ðŸ—‘ï¸ Deleted {deleted_count}")

            if result_parts:
                total_ops = created_count + updated_count + deleted_count
                suffix = "memory" if total_ops == 1 else "memories"
                message = ", ".join(result_parts) + f" {suffix}"
                await self._emit_status(emitter, message, True)
        else:
            await self._emit_status(emitter, "âœ… Memories already optimally organized", True)
        
        if total_executed > 0:
            await self._invalidate_user_cache(user_id, "consolidation")

    async def _consolidation_pipeline_task(self, user_message: str, conversation_context: str, user_id: str, emitter: Optional[Callable[[Any], Awaitable[None]]] = None) -> None:
        """Complete Consolidation Pipeline as async background task (formerly Slow Path)."""
        logger.info("ðŸ”§ Starting Consolidation Pipeline analysis")
        await self._emit_status(emitter, "ðŸ”§ Analyzing memory patterns...", False)
        
        candidates = await self._gather_consolidation_candidates(user_message, user_id, emitter)
        
        if not candidates:
            logger.info("ðŸ”§ Consolidation Pipeline: No candidates found")
            await self._emit_status(emitter, "ðŸ’­ No consolidation candidates", True)
            return
        
        operations = await self._llm_consolidate_memories(user_message, conversation_context, candidates, emitter)
        
        if not operations:
            logger.info("ðŸ”§ Consolidation Pipeline: Memories already optimally organized - no consolidation required")
            await self._emit_status(emitter, "âœ… Memories already well organized", True)
            return
        
        await self._execute_consolidation_operations(operations, user_id, emitter)
        
        logger.info("ðŸ”§ Consolidation Pipeline completed successfully")


    async def inlet(
        self,
        body: Dict[str, Any],
        __event_emitter__: Optional[Callable[[Any], Awaitable[None]]] = None,
        __user__: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """
        RETRIEVAL PIPELINE (Synchronous, formerly Fast Path)
        Process incoming messages to inject relevant memories into context.
        """
        if not body or not __user__:
            return body

        if "messages" in body and body["messages"]:
            last_user_msg = next(
                (
                    self._extract_text_from_message_content(m["content"])
                    for m in reversed(body.get("messages", []))
                    if m.get("role") == "user"
                ),
                None,
            )
            
            if not last_user_msg:
                return body
                
            user_message = last_user_msg
            user_id = __user__["id"]

            should_skip, skip_reason = self._should_skip_memory_operations(user_message)
            if should_skip:
                logger.info(f"â­ï¸ Retrieval Pipeline skipped: {skip_reason}")
                await self._emit_status(__event_emitter__, f"â­ï¸ Skipping: {skip_reason}", True)
                self._inject_datetime_context(body)
                return body

            await self._emit_status(__event_emitter__, "ðŸš€ Retrieving relevant memories...", False)
            
            candidate_memories = await self._broad_retrieval(user_message, user_id, __event_emitter__)
            
            if not candidate_memories:
                await self._emit_status(__event_emitter__, "ðŸ’­ No relevant memories", True)
                self._inject_datetime_context(body)
                return body
            
            relevant_memories = await self._llm_rerank_memories(user_message, candidate_memories, __event_emitter__)
            
            if relevant_memories:
                count = len(relevant_memories)
                await self._emit_status(__event_emitter__, f"ðŸ’¡ Injected {count} memories", True)
                self._inject_memories_into_context(body, relevant_memories, __event_emitter__)
            else:
                await self._emit_status(__event_emitter__, "ðŸ’­ No memories selected", True)

        self._inject_datetime_context(body)
        return body

    async def outlet(
        self,
        body: dict,
        __event_emitter__: Optional[Callable[[Any], Awaitable[None]]] = None,
        __user__: Optional[dict] = None,
    ) -> dict:
        """
        CONSOLIDATION PIPELINE (Asynchronous, formerly Slow Path)
        Process outgoing messages to consolidate and optimize memory database.
        """
        if not body or not __user__:
            return body

        if "messages" in body and body["messages"]:
            user_message = next(
                (
                    self._extract_text_from_message_content(m["content"])
                    for m in reversed(body["messages"])
                    if m["role"] == "user"
                ),
                None,
            )

            if user_message:
                should_skip, skip_reason = self._should_skip_memory_operations(user_message)
                if should_skip:
                    logger.info(f"â­ï¸ Consolidation Pipeline skipped: {skip_reason}")
                    return body

                conversation_context = ""
                for message in body["messages"]:
                    role = message.get("role", "")
                    content = self._extract_text_from_message_content(message.get("content", ""))
                    if content:
                        conversation_context += f"[{role}]: {content}\n"

                user_id = __user__["id"]

                logger.info("ðŸ”§ Starting Consolidation Pipeline background consolidation")
                task = asyncio.create_task(
                    self._consolidation_pipeline_task(user_message, conversation_context, user_id, __event_emitter__)
                )
                task.add_done_callback(
                    lambda t: logger.error(f"âŒ Consolidation pipeline task failed: {t.exception()}") 
                    if t.exception() else None
                )

        return body


    async def _invalidate_user_cache(self, user_id: str, reason: str = "") -> None:
        """Invalidate all cache entries for a specific user."""
        if Filter._cache_lock is None:
            Filter._cache_lock = asyncio.Lock()
            
        async with Filter._cache_lock:
            if user_id in Filter._embedding_cache:
                user_cache = Filter._embedding_cache[user_id]
                await user_cache.clear()
                logger.info(f"ðŸ§¹ Cache cleared for user {user_id}")
                
                if user_id in Filter._cache_access_order:
                    Filter._cache_access_order.remove(user_id)
                    Filter._cache_access_order.append(user_id)

    def _clean_memory_content(self, content: str) -> str:
        """Clean memory content and validate length limits."""
        clean_content = content.strip()
        if len(clean_content) > Config.MAX_MEMORY_CONTENT_LENGTH:
            raise ValueError(f"Memory content too long ({len(clean_content)} chars, max {Config.MAX_MEMORY_CONTENT_LENGTH})")
        return clean_content

    async def _execute_database_operation(self, operation_func, *args, timeout: float = None) -> Any:
        """Execute database operation with timeout and error handling."""
        if timeout is None:
            timeout = Config.TIMEOUT_DATABASE_OPERATION
        
        return await asyncio.wait_for(
            asyncio.to_thread(operation_func, *args),
            timeout=timeout,
        )

    async def _execute_single_operation(self, operation: MemoryOperation, user: Any) -> str:
        """Execute a single memory operation."""
        if operation.operation == "CREATE":
            clean_content = self._clean_memory_content(operation.content)
            
            await self._execute_database_operation(
                Memories.insert_new_memory, user.id, clean_content
            )
            logger.info(f"âœ¨ Memory created")
            return "CREATE"

        elif operation.operation == "UPDATE" and operation.id:
            clean_content = self._clean_memory_content(operation.content)

            await self._execute_database_operation(
                Memories.update_memory_by_id_and_user_id,
                operation.id,
                user.id,
                clean_content
            )
            logger.info(f"ðŸ”„ Memory updated {operation.id}")
            return "UPDATE"

        elif operation.operation == "DELETE" and operation.id:
            await self._execute_database_operation(
                Memories.delete_memory_by_id_and_user_id,
                operation.id,
                user.id
            )
            logger.info(f"ðŸ—‘ï¸ Memory deleted {operation.id}")
            return "DELETE"

        else:
            raise MemoryOperationError(f"Unsupported operation: {operation}")

    async def _query_llm(self, system_prompt: str, user_prompt: str, json_response: bool = True) -> str:
        """Query the OpenAI API or compatible endpoints with production-ready error handling."""
        if not self.valves.api_key or not self.valves.api_key.strip():
            raise NeuralRecallError("API key is required but not provided.")

        session = await self._get_aiohttp_session()
        url = f"{self.valves.api_url.rstrip('/')}/chat/completions"

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.valves.api_key}",
            "Accept": "application/json",
        }

        payload = {
            "model": self.valves.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "temperature": 0.1,
            "top_p": 0.9,
            "max_tokens": 2048,
        }

        if json_response:
            payload["response_format"] = {"type": "json_object"}

        try:
            async with session.post(url, json=payload, headers=headers) as response:
                if response.status == 429:
                    logger.warning("ðŸš¦ Rate limited by LLM API, retrying after delay")
                    await asyncio.sleep(1)
                    raise NeuralRecallError("Rate limited by API")
                
                if response.status == 503:
                    logger.warning("ðŸš« LLM service unavailable")
                    raise NeuralRecallError("LLM service unavailable")
                
                response.raise_for_status()
                data = await response.json()

                if "choices" in data and len(data["choices"]) > 0:
                    content = data["choices"][0]["message"]["content"]
                    if not content or content.strip() == "":
                        raise NeuralRecallError("Empty response from LLM")
                    return content

                raise NeuralRecallError(f"Unexpected API response format: {data}")
                
        except aiohttp.ClientError as e:
            logger.error(f"âŒ HTTP client error: {e}")
            raise NeuralRecallError(f"HTTP client error: {e}")
        except asyncio.TimeoutError:
            logger.error("âŒ LLM API timeout")
            raise NeuralRecallError("LLM API timeout")
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Invalid JSON response from LLM API: {e}")
            raise NeuralRecallError(f"Invalid JSON response: {e}")

    def _extract_and_parse_json(self, text: str) -> Any:
        """Extract and parse JSON from text response with production-ready error handling."""
        if not text or not text.strip():
            logger.warning("âš ï¸ Empty text provided for JSON parsing")
            return []

        text = text.strip()

        try:
            parsed = json.loads(text)
            if isinstance(parsed, dict):
                return [parsed]
            if isinstance(parsed, list):
                return parsed
            logger.warning(f"âš ï¸ Parsed JSON is not list/dict: {type(parsed)}")
            return []
        except json.JSONDecodeError:
            pass

        code_fence_match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL | re.IGNORECASE)
        if code_fence_match:
            inner = code_fence_match.group(1).strip()
            try:
                parsed = json.loads(inner)
                if isinstance(parsed, dict):
                    return [parsed]
                if isinstance(parsed, list):
                    return parsed
            except json.JSONDecodeError:
                logger.warning("âš ï¸ Failed to parse JSON inside ```json code fence")

        generic_fence = re.search(r'```\s*(.*?)\s*```', text, re.DOTALL)
        if generic_fence:
            inner = generic_fence.group(1).strip()
            try:
                parsed = json.loads(inner)
                if isinstance(parsed, dict):
                    return [parsed]
                if isinstance(parsed, list):
                    return parsed
            except json.JSONDecodeError:
                logger.warning("âš ï¸ Failed to parse JSON inside generic code fence")

        start = text.find('[')
        if start != -1:
            count = 0
            for i in range(start, len(text)):
                if text[i] == '[':
                    count += 1
                elif text[i] == ']':
                    count -= 1
                    if count == 0:
                        candidate = text[start:i+1]
                        try:
                            parsed = json.loads(candidate)
                            if isinstance(parsed, list):
                                return parsed
                        except json.JSONDecodeError:
                            break

        logger.warning("âš ï¸ Unable to extract valid JSON from LLM response; ensure the LLM returns only JSON (see consolidation prompt)")
        return []
    
    @classmethod
    async def cleanup(cls) -> None:
        """Cleanup resources for graceful shutdown (production requirement)."""
        try:
            if cls._aiohttp_session and not cls._aiohttp_session.closed:
                await cls._aiohttp_session.close()
                logger.info("âœ… HTTP session closed gracefully")
        except Exception as e:
            logger.error(f"âŒ Error during cleanup: {e}")
    
    def __del__(self):
        """Destructor to ensure cleanup (production safety)."""
        try:
            if hasattr(self, '_aiohttp_session') and self._aiohttp_session and not self._aiohttp_session.closed:
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        loop.create_task(self.cleanup())
                except:
                    pass
        except:
            pass
